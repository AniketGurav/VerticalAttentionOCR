{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "         aim of this part is to modify the code for word level attention\n",
    "    \n",
    "        1. Check the dataloader and create a dummy dataloader similar to it.\n",
    "    \n",
    "        2. Pass the input from dummy dataloader to model\n",
    "    \n",
    "        3. Modify the model\n",
    "    \n",
    "        4.Train the modified model on the actual data\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex not installed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/aniketag/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/')\n",
    "from torch.optim import Adam\n",
    "from OCR.document_OCR.v_attention.trainer_pg_va import Manager\n",
    "from OCR.document_OCR.v_attention.models_pg_va import VerticalAttention, LineDecoderCTC\n",
    "from basic.models import FCN_Encoder\n",
    "from basic.generic_dataset_manager import OCRDataset\n",
    "from basic.generic_training_manager import GenericTrainingManager\n",
    "from torch.nn import Flatten, LSTM, Embedding\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"IAM\"  # [\"RIMES\", \"IAM\", \"READ_2016\"]\n",
    "\n",
    "params = {\n",
    "    \"dataset_params\": {\n",
    "        \"datasets\": {\n",
    "            dataset_name: \"/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/{}_paragraph\".format(dataset_name),\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"name\": \"{}-train\".format(dataset_name),\n",
    "            \"datasets\": [dataset_name, ],\n",
    "        },\n",
    "        \"valid\": {\n",
    "            \"{}-valid\".format(dataset_name): [dataset_name, ],\n",
    "        },\n",
    "        \"dataset_class\": OCRDataset,\n",
    "        \"config\": {\n",
    "            \"width_divisor\": 8,  # Image width will be divided by 8\n",
    "            \"height_divisor\": 32,  # Image height will be divided by 32\n",
    "            \"padding_value\": 0,  # Image padding value\n",
    "            \"padding_token\": None,  # Label padding value (None: default value is chosen)\n",
    "            \"charset_mode\": \"CTC\",  # add blank label\n",
    "            \"constraints\": [\"padding\", \"CTC_va\"],  # Padding for models constraints and CTC requirements\n",
    "            \"padding\": {\n",
    "                \"min_height\": 480,  # to handle model requirements (AdaptivePooling)\n",
    "                \"min_width\": 800,  # to handle model requirements (AdaptivePooling)\n",
    "            },\n",
    "            \"preprocessings\": [\n",
    "                {\n",
    "                    \"type\": \"dpi\",  # modify image resolution\n",
    "                    \"source\": 300,  # from 300 dpi\n",
    "                    \"target\": 150,  # to 150 dpi\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"to_RGB\",\n",
    "                    # if grayscale image, produce RGB one (3 channels with same value) otherwise do nothing\n",
    "                },\n",
    "            ],\n",
    "            # Augmentation techniques to use at training time\n",
    "            \"augmentation\": {\n",
    "                \"dpi\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"min_factor\": 0.75,\n",
    "                    \"max_factor\": 1,\n",
    "                },\n",
    "                \"perspective\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"min_factor\": 0,\n",
    "                    \"max_factor\": 0.3,\n",
    "                },\n",
    "                \"elastic_distortion\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"max_magnitude\": 20,\n",
    "                    \"max_kernel\": 3,\n",
    "                },\n",
    "                \"random_transform\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"max_val\": 125,\n",
    "                },\n",
    "                \"dilation_erosion\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"min_kernel\": 1,\n",
    "                    \"max_kernel\": 3,\n",
    "                    \"iterations\": 1,\n",
    "                },\n",
    "                \"brightness\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"min_factor\": 0.01,\n",
    "                    \"max_factor\": 1,\n",
    "                },\n",
    "                \"contrast\": {\n",
    "                    \"proba\": 0.2,\n",
    "                    \"min_factor\": 0.01,\n",
    "                    \"max_factor\": 1,\n",
    "                },\n",
    "                \"sign_flipping\": {\n",
    "                    \"proba\": 0.2,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"model_params\": {\n",
    "        # Model classes to use for each module\n",
    "        \"models\": {\n",
    "            \"encoder\": FCN_Encoder,\n",
    "            \"attention\": VerticalAttention,\n",
    "            \"decoder\": LineDecoderCTC,\n",
    "        },\n",
    "        \"transfer_learning\": None,\n",
    "        # \"transfer_learning\": {\n",
    "        #     # model_name: [state_dict_name, checkpoint_path, learnable, strict]\n",
    "        #     \"encoder\": [\"encoder\", \"../../line_OCR/ctc/outputs/iam/checkpoints/best_XX.pt\", True, True],\n",
    "        #     \"decoder\": [\"decoder\", \"../../line_OCR/ctc/outputs/iam/checkpoints/best_XX.pt\", True, True],\n",
    "        #\n",
    "        # },\n",
    "        \"input_channels\": 3,  # 3 for RGB images, 1 for grayscale images\n",
    "\n",
    "        # dropout probability for standard dropout (half dropout probability is taken for spatial dropout)\n",
    "        \"dropout\": 0.5,  # dropout for encoder module\n",
    "        \"dec_dropout\": 0.5,  # dropout for decoder module\n",
    "        \"att_dropout\": 0,  # dropout for attention module\n",
    "\n",
    "        \"features_size\": 256,  # encoder output features maps\n",
    "        \"att_fc_size\": 256,  # number of channels for attention sum computation\n",
    "\n",
    "        \"use_location\": True,  # use previous attention weights in attention module\n",
    "        \"use_coverage_vector\": True,  # use coverage vector in attention module\n",
    "        \"coverage_mode\": \"clamp\",  # mode to use for the coverage vector\n",
    "\n",
    "        \"emb_max_features_width\": 250,  # maximum feature width (for use_abs_position)\n",
    "        \"emb_max_features_height\": 100,  # maximum feature height (for use_abs_position)\n",
    "\n",
    "        \"use_hidden\": True,  # use decoder hidden state in attention (and thus LSTM in decoder)\n",
    "        \"hidden_size\": 256,  # number of cells for LSTM decoder hidden state\n",
    "        \"nb_layers_decoder\": 1,  # number of layers for LSTM decoder\n",
    "\n",
    "        \"min_height_feat\": 15,  # min height for attention module (AdaptivePooling)\n",
    "        \"min_width_feat\": 100,  # min width for attention module (AdaptivePooling)\n",
    "    },\n",
    "\n",
    "    \"training_params\": {\n",
    "        \"output_folder\": \"van_iam_paragraph_learned_stop\",  # folder names for logs and weigths\n",
    "        \"max_nb_epochs\": 5000,  # max number of epochs for the training\n",
    "        \"max_training_time\": 3600 * (24 + 23),  # max training time limit (in seconds)\n",
    "        \"load_epoch\": \"best\",  # [\"best\", \"last\"], to load weights from best epoch or last trained epoch\n",
    "        \"interval_save_weights\": None,  # None: keep best and last only\n",
    "        \"batch_size\": 1,  # mini-batch size per GPU\n",
    "        \"use_ddp\": False,  # Use DistributedDataParallel\n",
    "        \"ddp_port\": \"10000\",  # Port for Distributed Data Parallel communications\n",
    "        \"use_apex\": True,  # Enable mix-precision with apex package\n",
    "        \"nb_gpu\": torch.cuda.device_count(),\n",
    "        \"optimizer\": {\n",
    "            \"class\": Adam,\n",
    "            \"args\": {\n",
    "                \"lr\": 0.0001,\n",
    "                \"amsgrad\": False,\n",
    "            }\n",
    "        },\n",
    "        \"eval_on_valid\": True,  # Whether to eval and logs metrics on validation set during training or not\n",
    "        \"eval_on_valid_interval\": 2,  # Interval (in epochs) to evaluate during training\n",
    "        \"focus_metric\": \"cer\",  # Metrics to focus on to determine best epoch\n",
    "        \"expected_metric_value\": \"low\",  # [\"high\", \"low\"] What is best for the focus metric value\n",
    "        \"set_name_focus_metric\": \"{}-valid\".format(dataset_name),\n",
    "        \"train_metrics\": [\"loss_ctc\", \"cer\", \"wer\"],  # Metrics name for training\n",
    "        \"eval_metrics\": [\"cer\", \"wer\", \"diff_len\"],  # Metrics name for evaluation on validation set during training\n",
    "        \"force_cpu\": False,  # True for debug purposes to run on cpu only\n",
    "        \"max_pred_lines\": 30,  # Maximum number of line predictions at evaluation time\n",
    "        \"stop_mode\": \"learned\",  # [\"fixed\", \"early\", \"learned\"]\n",
    "\n",
    "    },\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniTrain(GenericTrainingManager):\n",
    "    def __init__(self,params) -> None:\n",
    "        \n",
    "        self.params = params\n",
    "        \n",
    "        print(\"\\n\\t inside minitrain!!\")\n",
    "        \n",
    "        self.load_dataset()\n",
    "        self.load_model()\n",
    "    \n",
    "    def testdata():\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t inside minitrain!!\n",
      "\n",
      "\t stamp 1\n",
      "\n",
      "\t datasets[key] = /media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph/\n",
      "\n",
      "\t joinPath = /media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph/labels.pkl\n",
      "\n",
      "\t is file: True\n",
      "\n",
      "\t stamp 1.1\n",
      "\n",
      "\t stamp 1.3\n",
      "\n",
      "\t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'train'}]\n",
      "\n",
      "\t from_segmentation: False  \t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'train'}]\n",
      "\n",
      "\t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'train'}]\n",
      "\n",
      "\t keys: dict_keys(['name', 'label', 'img', 'unchanged_label', 'raw_line_seg_label'])\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_0.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_1.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_2.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_3.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_4.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_5.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_6.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_7.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_8.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_9.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_10.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_11.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_12.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_13.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_14.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_15.png\n",
      "\n",
      "\t self.samples[i][name]: IAM_paragraph/train/train_16.png\n",
      "\n",
      "\t stamp 1.3.1\n",
      "\n",
      "\t stamp 1.4.1 [239.41648798, 239.41648798, 239.41648798] [34.83986196, 34.83986196, 34.83986196]\n",
      "\n",
      "\t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'valid'}]\n",
      "\n",
      "\t from_segmentation: False  \t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'valid'}]\n",
      "\n",
      "\t paths_and_sets: [{'path': '/media/aniketag/c4eb0693-4a65-4f0c-8d65-a6dad4b97ff9/IAM/formatted/IAM_paragraph', 'set_name': 'valid'}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3846198/3278373687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mminiTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3846198/3651776048.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t inside minitrain!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_training_manager.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t stamp 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t stamp 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_dataset_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_merged_charsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t stamp 1.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t stamp 1.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_dataset_manager.py\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcustom_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcustom_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_paths_and_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcustom_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m#print(\"\\n\\t stamp 1.4.2\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_dataset_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, set_name, custom_name, paths_and_sets)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_and_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOCRDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_and_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_dataset_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, set_name, custom_name, paths_and_sets, from_segmentation)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_and_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preprocessings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"padding_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/basic/generic_dataset_manager.py\u001b[0m in \u001b[0;36mapply_preprocessing\u001b[0;34m(self, preprocessings)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\t keys:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "miniTrain(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 121, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate a random context vector for demonstration\n",
    "context_vector = torch.randn(1, 256, 121)\n",
    "\n",
    "batch_size = context_vector.size(0)\n",
    "max_line_len = context_vector.size(2)\n",
    "input_size = context_vector.size(1)\n",
    "hidden_size = 64\n",
    "\n",
    "# Reshape the context vector\n",
    "context_vector = context_vector.transpose(1, 2).reshape(batch_size, max_line_len, input_size)\n",
    "\n",
    "# Apply a linear layer to obtain a new tensor with shape (batch_size, max_line_len, hidden_size)\n",
    "linear_layer = torch.nn.Linear(input_size, hidden_size)\n",
    "context_vector = linear_layer(context_vector)\n",
    "\n",
    "# Apply softmax along the second dimension\n",
    "weights = torch.nn.functional.softmax(context_vector, dim=1)\n",
    "\n",
    "# Check the shape of the weights tensor\n",
    "print(weights.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniketag/anaconda3/envs/tor11/lib/python3.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: An exception was ignored while fetching the attribute `__array_interface__` from an object of type 'PngImageFile'.  With the exception of `AttributeError` NumPy will always raise this exception in the future.  Raise this deprecation warning to see the original exception. (Warning added NumPy 1.21)\n"
     ]
    }
   ],
   "source": [
    "samples = load_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tanh, log_softmax, softmax, relu\n",
    "from torch.nn import Conv1d, Conv2d, Dropout,  Linear, AdaptiveMaxPool2d, InstanceNorm1d, AdaptiveMaxPool1d\n",
    "from torch.nn import Flatten, LSTM, Embedding\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/aniketag/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/')\n",
    "from torch.optim import Adam\n",
    "from OCR.document_OCR.v_attention.trainer_pg_va import Manager\n",
    "#from OCR.document_OCR.v_attention.models_pg_va import VerticalAttention, LineDecoderCTC\n",
    "from basic.models import FCN_Encoder\n",
    "from basic.generic_dataset_manager import OCRDataset\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "#from parameters import params\n",
    "\n",
    "class BahdanauAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W1 = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        # encoder_outputs shape: (batch_size, max_line_len, hidden_size)\n",
    "\n",
    "        # Compute attention scores\n",
    "        hidden = hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        W1_hidden = self.W1(hidden)  # (batch_size, 1, hidden_size)\n",
    "        W2_encoder = self.W2(encoder_outputs)  # (batch_size, max_line_len, hidden_size)\n",
    "        scores = self.V(torch.tanh(W1_hidden + W2_encoder))  # (batch_size, max_line_len, 1)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # (batch_size, max_line_len, 1)\n",
    "\n",
    "        # Compute context vector\n",
    "        context_vector = (attention_weights * encoder_outputs).sum(dim=1)  # (batch_size, hidden_size)\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'use_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_982330/118216640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLineDecoderCTC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/OCR/document_OCR/v_attention/models_pg_va.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_hidden\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'use_hidden'"
     ]
    }
   ],
   "source": [
    "LineDecoderCTC(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tanh, log_softmax, softmax, relu\n",
    "from torch.nn import Conv1d, Conv2d, Dropout,  Linear, AdaptiveMaxPool2d, InstanceNorm1d, AdaptiveMaxPool1d\n",
    "from torch.nn import Flatten, LSTM, Embedding\n",
    "from basic.models import DepthSepConv2D\n",
    "\n",
    "class LineDecoderCTC(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(LineDecoderCTC, self).__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.hidden_size = 256 #params[\"hidden_size\"]\n",
    "        self.batch_size = 2\n",
    "        self.max_line_len = 10 # assuming there will be max 10 words in a line\n",
    "\n",
    "        self.hidden = torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "        self.use_hidden = True #params[\"use_hidden\"]\n",
    "        self.input_size = 256 #params[\"features_size\"]\n",
    "        self.vocab_size = 79 #params[\"vocab_size\"]\n",
    "        self.attention = BahdanauAttention1(self.hidden_size)\n",
    "        self.word_context_vectors = []\n",
    "        \n",
    "        self.context_vector2 = None\n",
    "        self.context_vector = None \n",
    "        self.attention_weights = None\n",
    "        \n",
    "        self.wrdCon2Dcd = nn.Linear(256,self.input_size) # this will take input word context vector and\n",
    "                                                             # converts it to the shape that LSTM can process \n",
    "        \n",
    "        \n",
    "        self.decoder = RNNDecoder(self.hidden_size,1)\n",
    "\n",
    "        \n",
    "        if self.use_hidden:\n",
    "            \n",
    "            self.lstm = LSTM(self.input_size, self.hidden_size, num_layers=1)\n",
    "            self.end_conv = Conv2d(in_channels=self.hidden_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "            \n",
    "            \"\"\"\n",
    "            self.lstm1 = LSTM(self.input_size, self.hidden_size, num_layers=1)\n",
    "            self.end_conv1 = Conv2d(in_channels=self.hidden_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.end_conv = Conv2d(in_channels=self.input_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "        \n",
    "\n",
    "        self.linear1 = torch.nn.Linear(256, 256)\n",
    "            \n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "                        \n",
    "                \n",
    "        \"\"\"\n",
    "        x (B, C, W)\n",
    "        \"\"\"\n",
    "        \n",
    "        x1 = x.clone() # torch.Size([1, 256, 116])\n",
    "        x1 = x1.permute(0,2,1)\n",
    "        #print(\"\\n\\t x1.shape:\",x1.shape)\n",
    "        \n",
    "        hidden_rep = self.linear1(x1)\n",
    "        \n",
    "        print(\"\\n\\t 1.x.shape:\",x1.shape,\"\\t x.shape:\",x.shape,\"\\t hidden_rep.shape:\",hidden_rep.shape)        \n",
    "        #           1.x1.shape: torch.Size([2, 116, 256]) \t x.shape: torch.Size([2, 256, 116]) \t hidden_rep.shape: torch.Size([2, 116, 256])\n",
    "\n",
    "        print(\"\\n\\t self.hidden:\",self.hidden.shape) # torch.Size([2, 256])\n",
    "        for i in range(self.max_line_len):\n",
    "            print(\"\\n\\t i:\",i)\n",
    "\n",
    "            self.context_vector, self.context_vector2, self.attention_weights = self.attention(self.hidden, hidden_rep)\n",
    "\n",
    "            #########################################################################################################\n",
    "            print(\"\\n\\t word context_vector:\",self.context_vector.permute(2, 0, 1).shape,\" \\t attention_weights.shape:\",self.attention_weights.shape)\n",
    "            print(\"\\n\\t word context_vector2:\",self.context_vector2.shape) #  torch.Size([batch_size, 256])\n",
    "            \n",
    "            \n",
    "            # \t word context_vector: torch.Size([2, 256])  \t attention_weights.shape: torch.Size([2, 116, 1]) (old)\n",
    "            #  word context_vector: torch.Size([2, 256, 116])  \t attention_weights.shape: torch.Size([2, 116])     (new)\n",
    "            \n",
    "                        \n",
    "            xOut, hOut = self.lstm(self.context_vector.permute(2, 0, 1), h) \n",
    "            xOut = xOut.permute(1, 2, 0)\n",
    "\n",
    "            print(\"\\n\\t xOut:\",xOut.shape,\"\\t hOut.shape =\",hOut[0].shape)\n",
    "\t        #  \t word context_vector: torch.Size([2, 256, 116])  \t attention_weights.shape: torch.Size([2, 116])    \n",
    "            self.word_context_vectors.append(self.context_vector)\n",
    "\n",
    "            temp3 = xOut\n",
    "            out2 = self.end_conv(temp3.unsqueeze(3)).squeeze(3)\n",
    "            print(\"\\n\\t out2.shape:\",out2.shape)\n",
    "            \n",
    "            #########################################################################################################\n",
    "\n",
    "            #########################################################################################################\n",
    "            \n",
    "            print(\"\\n\\t self.hidden.shape before:\",self.hidden.shape)\n",
    "            self.hidden = self.decoder(self.hidden,self.context_vector2) \n",
    "            print(\"\\n\\t self.hidden.shape after:\",self.hidden.shape)\n",
    "\n",
    "            \n",
    "            #########################################################################################################\n",
    "\n",
    "            \n",
    "        if 1:#self.use_hidden:\n",
    "            \n",
    "            print(\"\\n\\t 1.1.0.x.shape:\",x.permute(2, 0, 1).shape) #  1.1.0.x.shape: torch.Size([116, 2, 256])         \n",
    "\n",
    "            x, h = self.lstm(x.permute(2, 0, 1), h) \n",
    "            print(\"\\n\\t 1.1.1. x.shape:\",x.shape,\" \\t h.shape:\",h[0].shape)    \n",
    "            #  1.1.1. x.shape: torch.Size([116, 2, 256]), h.shape: torch.Size([1, 2, 256]) middle dim 2 is batch size in both vectors    \n",
    "\n",
    "            x = x.permute(1, 2, 0)\n",
    "\n",
    "        temp2 = x.unsqueeze(3)\n",
    "        print(\"\\n\\t 2.x.shape:\",x.shape,\" temp2.shape:\",temp2.shape)        \n",
    "        # 2.x.shape: torch.Size([2, 256, 116])  temp2.shape: torch.Size([2, 256, 116, 1])\n",
    "        \n",
    "        out = self.end_conv(x.unsqueeze(3)).squeeze(3)\n",
    "        print(\"\\n\\t out1.shape:\",out.shape,\"\\t out2.shape:\",out2.shape)\n",
    "        \n",
    "        out = torch.squeeze(out, dim=2)\n",
    "        out = log_softmax(out, dim=1)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, drop=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(embed_size, embed_size, num_layers)\n",
    "        if self.num_layers > 1: self.rnn.dropout = drop\n",
    "\n",
    "    def forward(self, hidden, context):\n",
    "        _, h = self.rnn(context.unsqueeze(0), hidden.expand(self.num_layers, -1, -1).contiguous())\n",
    "\n",
    "        return h[-1]\n",
    "\n",
    "class DeepOutputLayer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(embed_size*3, embed_size)\n",
    "        self.l2 = nn.Linear(embed_size, vocab_size)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, prev, hidden, context):\n",
    "        # this is called once for each timestep\n",
    "        #(30,256)\n",
    "        out = self.l1(torch.cat([prev,hidden,context], -1))\n",
    "        out = self.l2(self.drop(F.leaky_relu(out)))\n",
    "        return out\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model, drop=0.2):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.lut(x) * math.sqrt(self.d_model)\n",
    "        return self.drop(emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t 1.x.shape: torch.Size([2, 116, 256]) \t x.shape: torch.Size([2, 256, 116]) \t hidden_rep.shape: torch.Size([2, 116, 256])\n",
      "\n",
      "\t self.hidden: torch.Size([2, 256])\n",
      "\n",
      "\t i: 0\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 1\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 2\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 3\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 4\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 5\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 6\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 7\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 8\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t i: 9\n",
      "\n",
      "\t attention_weights = torch.Size([2, 116, 1])\n",
      "\n",
      "\t word context_vector: torch.Size([116, 2, 256])  \t attention_weights.shape: torch.Size([2, 116])\n",
      "\n",
      "\t word context_vector2: torch.Size([2, 256])\n",
      "\n",
      "\t xOut: torch.Size([2, 256, 116]) \t hOut.shape = torch.Size([1, 2, 256])\n",
      "\n",
      "\t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t self.hidden.shape before: torch.Size([2, 256])\n",
      "\n",
      "\t self.hidden.shape after: torch.Size([2, 256])\n",
      "\n",
      "\t 1.1.0.x.shape: torch.Size([116, 2, 256])\n",
      "\n",
      "\t 1.1.1. x.shape: torch.Size([116, 2, 256])  \t h.shape: torch.Size([1, 2, 256])\n",
      "\n",
      "\t 2.x.shape: torch.Size([2, 256, 116])  temp2.shape: torch.Size([2, 256, 116, 1])\n",
      "\n",
      "\t out1.shape: torch.Size([2, 80, 116]) \t out2.shape: torch.Size([2, 80, 116])\n",
      "\n",
      "\t out.shape: torch.Size([2, 80, 116])\n",
      "\t h.shape 2\n",
      "1. torch.Size([1, 2, 256])\n",
      "1. torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contextVect = torch.rand([2,256,116])\n",
    "\n",
    "\n",
    "decoder = LineDecoderCTC(params)\n",
    "\n",
    "out,h = decoder(contextVect)\n",
    "\n",
    "print(\"\\n\\t out.shape:\",out.shape) \n",
    "print(\"\\t h.shape\",len(h))\n",
    "\n",
    "print(\"1.\",h[0].shape)\n",
    "\n",
    "print(\"1.\",h[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modified attention to handle context vector \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BahdanauAttention1(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W1 = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        # encoder_outputs shape: (batch_size, max_line_len, hidden_size)\n",
    "\n",
    "        # Compute attention scores\n",
    "        hidden = hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        W1_hidden = self.W1(hidden)  # (batch_size, 1, hidden_size)\n",
    "        W2_encoder = self.W2(encoder_outputs)  # (batch_size, max_line_len, hidden_size)\n",
    "        scores = self.V(torch.tanh(W1_hidden + W2_encoder))  # (batch_size, max_line_len, 1)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # (batch_size, max_line_len, 1)\n",
    "        print(\"\\n\\t attention_weights =\",attention_weights.shape)\n",
    "        \n",
    "        #attention_weights = attention_weights.squeeze(2)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context_vector = (attention_weights * encoder_outputs).sum(dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        context_vector2 = context_vector.clone()\n",
    "        \n",
    "        context_vector = context_vector.unsqueeze(0).transpose(1, 2).repeat(1, encoder_outputs.shape[1], 1, 1)\n",
    "        context_vector = context_vector.squeeze(0)\n",
    "        context_vector = context_vector.permute(2,1,0) # torch.Size([3, 256, 116])\n",
    "        \n",
    "        \n",
    "        # context_vector shape: (1, max_line_len, batch_size, hidden_size)\n",
    "\n",
    "        return context_vector,context_vector2, attention_weights.squeeze(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t attention_weights = torch.Size([12, 116, 1])\n",
      "context_vector shape: torch.Size([12, 256, 116])  \t cv2.shape: torch.Size([12, 256])\n",
      "attention_weights shape: torch.Size([12, 116])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# define batch size, max line length, and hidden size\n",
    "batch_size = 12\n",
    "max_line_len = 116\n",
    "hidden_size = 256\n",
    "\n",
    "# create random encoder_outputs tensor\n",
    "encoder_outputs = torch.randn(batch_size, max_line_len, hidden_size)\n",
    "\n",
    "# create random hidden tensor\n",
    "hidden = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "# create BahdanauAttention object\n",
    "attention1 = BahdanauAttention1(hidden_size)\n",
    "\n",
    "# call forward method to get context_vector and attention_weights\n",
    "context_vector,context_vector2, attention_weights = attention1(hidden, encoder_outputs)\n",
    "\n",
    "print(\"context_vector shape:\", context_vector.shape,\" \\t cv2.shape:\",context_vector2.shape)  # should output (batch_size, hidden_size)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)  # should output (batch_size, max_line_len, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex not installed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/aniketag/Documents/phd/TensorFlow-2.x-YOLOv3_simula/Handwriting-1-master/VerticalAttentionOCR/')\n",
    "from torch.optim import Adam\n",
    "from OCR.document_OCR.v_attention.trainer_pg_va import Manager\n",
    "from OCR.document_OCR.v_attention.models_pg_va import VerticalAttention, LineDecoderCTC\n",
    "from basic.models import FCN_Encoder\n",
    "from basic.generic_dataset_manager import OCRDataset\n",
    "from basic.generic_training_manager import GenericTrainingManager\n",
    "from torch.nn import Flatten, LSTM, Embedding\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tanh, log_softmax, softmax, relu\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Conv1d, Conv2d, Dropout,  Linear, AdaptiveMaxPool2d, InstanceNorm1d, AdaptiveMaxPool1d\n",
    "from torch.nn import Flatten, LSTM, Embedding\n",
    "\n",
    "\n",
    "\n",
    "from basic.models import DepthSepConv2D\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, CTCLoss\n",
    "from basic.generic_dataset_manager import DatasetManager\n",
    "\n",
    "\n",
    "#dataset = DatasetManager(params[\"dataset_params\"])\n",
    "\n",
    "\n",
    "loss_ctc_func = CTCLoss(79, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, drop=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(embed_size, embed_size, num_layers)\n",
    "        if self.num_layers > 1: self.rnn.dropout = drop\n",
    "\n",
    "    def forward(self, hidden, context):\n",
    "        _, h = self.rnn(context.unsqueeze(0), hidden.expand(self.num_layers, -1, -1).contiguous())\n",
    "\n",
    "        return h[-1]\n",
    "\n",
    "class DeepOutputLayer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(embed_size*2, embed_size)\n",
    "        self.l2 = nn.Linear(embed_size, vocab_size+1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, hidden, context):\n",
    "        # this is called once for each timestep\n",
    "        #(30,256)\n",
    "        out = self.l1(torch.cat([hidden,context], -1))\n",
    "        out = self.l2(self.drop(F.leaky_relu(out)))\n",
    "        return out\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model, drop=0.2):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.lut(x) * math.sqrt(self.d_model)\n",
    "        return self.drop(emb)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class BahdanauAttention1(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W1 = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        # encoder_outputs shape: (batch_size, max_line_len, hidden_size)\n",
    "\n",
    "        # Compute attention scores\n",
    "        hidden = hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        W1_hidden = self.W1(hidden)  # (batch_size, 1, hidden_size)\n",
    "        W2_encoder = self.W2(encoder_outputs)  # (batch_size, max_line_len, hidden_size)\n",
    "        \n",
    "        #print(\"\\n\\t W1_hidden.shape:\",W1_hidden.shape,\"\\t W2_encoder.shape:\",W2_encoder.shape)#,\"\\t self.batch_size:\",self.batch_size)\n",
    "\n",
    "        \n",
    "        scores = self.V(torch.tanh(W1_hidden + W2_encoder))  # (batch_size, max_line_len, 1)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # (batch_size, max_line_len, 1)\n",
    "        #print(\"\\n\\t attention_weights =\",attention_weights.shape)\n",
    "        \n",
    "        #attention_weights = attention_weights.squeeze(2)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context_vector = (attention_weights * encoder_outputs).sum(dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        context_vector2 = context_vector.clone()\n",
    "        \n",
    "        context_vector = context_vector.unsqueeze(0).transpose(1, 2).repeat(1, encoder_outputs.shape[1], 1, 1)\n",
    "        context_vector = context_vector.squeeze(0)\n",
    "        context_vector = context_vector.permute(2,1,0) # torch.Size([3, 256, 116])\n",
    "        \n",
    "        \n",
    "        # context_vector shape: (1, max_line_len, batch_size, hidden_size)\n",
    "\n",
    "        return context_vector,context_vector2, attention_weights.squeeze(2)\n",
    "\n",
    "\n",
    "class LineDecoderCTC1(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(LineDecoderCTC1, self).__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.hidden_size = 256 #params[\"hidden_size\"]\n",
    "        self.batch_size =  params[\"training_params\"][\"batch_size\"] # 2\n",
    "        self.max_line_len = 150 # assuming there will be max 10 words in a line\n",
    "\n",
    "        self.hidden = torch.zeros(self.batch_size, self.hidden_size).to(\"cuda\")\n",
    "\n",
    "        self.use_hidden = True #params[\"use_hidden\"]\n",
    "        self.input_size = 256 #params[\"features_size\"]\n",
    "        self.vocab_size = 79 #params[\"vocab_size\"]\n",
    "        self.attention = BahdanauAttention1(self.hidden_size).to(\"cuda:0\")\n",
    "        self.word_context_vectors = []\n",
    "        self.dec_inp = []\n",
    "        \n",
    "        self.context_vector2 = None\n",
    "        self.context_vector = None \n",
    "        self.attention_weights = None\n",
    "        \n",
    "        self.wrdCon2Dcd = Linear(256,self.input_size) # this will take input word context vector and\n",
    "                                                             # converts it to the shape that LSTM can process \n",
    "\n",
    "        self.decoder = RNNDecoder(self.hidden_size,1).to(\"cuda:0\")\n",
    "        self.output  = DeepOutputLayer(self.hidden_size, self.vocab_size).to(\"cuda:0\")\n",
    "                \n",
    "        if self.use_hidden:\n",
    "            \n",
    "            self.lstm = LSTM(self.input_size, self.hidden_size, num_layers=1)\n",
    "            self.end_conv = Conv2d(in_channels=self.hidden_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "            \n",
    "            \"\"\"\n",
    "            self.lstm1 = LSTM(self.input_size, self.hidden_size, num_layers=1)\n",
    "            self.end_conv1 = Conv2d(in_channels=self.hidden_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.end_conv = Conv2d(in_channels=self.input_size, out_channels=self.vocab_size + 1, kernel_size=1)\n",
    "        \n",
    "\n",
    "        self.linear1 = Linear(256, 256).to(\"cuda:0\")\n",
    "            \n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "                        \n",
    "                \n",
    "        \"\"\"\n",
    "        x (B, C, W)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        res,attns = [],[]\n",
    "        \n",
    "        maxChars = x.shape[2]\n",
    "        \n",
    "        x1 = x.clone() # torch.Size([1, 256, 116])\n",
    "\n",
    "        \n",
    "        x1 = x1.to(\"cuda:0\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "        x1 = x1.permute(0,2,1) # [1, 116, 256]\n",
    "        \n",
    "        print(\"\\n\\t x1.shape:\",x1.shape)\n",
    "        #print(\"\\n\\t 1:\",x1.device,\" \\t x.device:\",x.device)\n",
    "        \n",
    "        hidden_rep = self.linear1(x1) # [1, 116, 256]\n",
    "\n",
    "        print(\"\\n\\t hidden_rep.shape:\",hidden_rep.shape,\" \\t maxChars:\",maxChars)\n",
    "\n",
    "\n",
    "        #print(\"\\n\\t 2\")\n",
    "        \n",
    "        #print(\"\\n\\t 1.x.shape:\",x1.shape,\"\\t x.shape:\",x.shape,\"\\t hidden_rep.shape:\",hidden_rep.shape)        \n",
    "        #           1.x1.shape: torch.Size([2, 116, 256]) \t x.shape: torch.Size([2, 256, 116]) \t hidden_rep.shape: torch.Size([2, 116, 256])\n",
    "\n",
    "        #print(\"\\n\\t self.hidden:\",self.hidden.shape) # torch.Size([2, 256])\n",
    "        \n",
    "        randLen = maxChars\n",
    "        \n",
    "        \"\"\"\n",
    "        res = torch.zeros([randLen,1,80]).to(\"cuda:0\")\n",
    "        res = res.permute(0,2,1)\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        \n",
    "        #print(\"\\n\\t x1.shape:\",x1.shape,\" \\t res =\",res.shape)\n",
    "        \n",
    "        #attns = torch.rand([randLen,1,randLen]).to(\"cuda:0\")\n",
    "        #res = torch.zeros([1, randLen, 80], dtype=torch.float32, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(maxChars):\n",
    "            #print(\"\\n\\t char i:\",i)\n",
    "                                                                                              #([1, 256],[1, 116, 256])\n",
    "            self.context_vector, self.context_vector2, self.attention_weights = self.attention(self.hidden, hidden_rep)\n",
    "            # [1, 256, 116]    , [1, 256]            , [1, 116]\n",
    "            encoder_outputs = hidden_rep.clone()\n",
    "            \n",
    "            #print(\"\\n\\t ii:\",self.context_vector.shape, self.context_vector2.shape, self.attention_weights.shape,encoder_outputs.shape)\n",
    "            # \t        ii: torch.Size([1, 256, 116]), torch.Size([1, 256]), torch.Size([1, 116]) torch.Size([1, 116, 256])\n",
    "\n",
    "            #print(\"\\n\\t word context_vector:\",self.context_vector.permute(2, 0, 1).shape,\" \\t attention_weights.shape:\",self.attention_weights.shape)\n",
    "            #print(\"\\n\\t word context_vector2:\",self.context_vector2.shape) #  torch.Size([batch_size, 256])\n",
    "\n",
    "            #print(\"\\n\\t self.hidden.shape before:\",self.hidden.shape) # torch.Size([1, 256])\n",
    "            \n",
    "            #                          ([1, 256], [1, 256])                             \n",
    "            self.hidden = self.decoder(self.hidden,self.context_vector2)  \n",
    "            #print(\"\\n\\t self.hidden.shape after:\",self.hidden.shape) # torch.Size([1, 256])\n",
    "            \n",
    "            #                          ([1, 256], [1, 256])\n",
    "            charOut = self.output(self.hidden, self.context_vector2)\n",
    "            #print(\"\\n\\t charOut.shape =\",charOut.shape) # charOut.shape = torch.Size([1, 80])\n",
    "            \n",
    "            charOut = log_softmax(charOut, dim=1)\n",
    "            \n",
    "            #print(\"\\n\\t charOut.shape:\",charOut.unsqueeze(dim=1).shape)\n",
    "            \n",
    "            #res = torch.cat((res[:, :i, :], charOut.unsqueeze(dim=1), res[:, i+1:, :]), dim=1)\n",
    "\n",
    "            res.append(charOut)\n",
    "            attns.append(self.attention_weights)\n",
    "            #dec_inp = charOut.data.max(1)[1]\n",
    "\n",
    "            self.dec_inp.append(charOut.data.max(1)[1])\n",
    "            \n",
    "            #############################################################################################################\n",
    "\n",
    "            \n",
    "        res = torch.stack(res)\n",
    "        attns = torch.stack(attns)\n",
    "        return res, attns, self.dec_inp\n",
    "\n",
    "ldc1 = LineDecoderCTC1(params).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t x1.shape: torch.Size([1, 116, 256])\n",
      "\n",
      "\t hidden_rep.shape: torch.Size([1, 116, 256])  \t maxChars: 116\n",
      "torch.Size([116, 1, 80]) torch.Size([116, 1, 116])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([1, 256, 116])\n",
    "x.shape\n",
    "\n",
    "res, attns, dec_inp = ldc1(x)\n",
    "\n",
    "print(res.shape, attns.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor11",
   "language": "python",
   "name": "tor11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
